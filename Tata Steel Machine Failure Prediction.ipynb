{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BQJ9s48ybAVen7UzZkLS0dqXqqJwWy9w","timestamp":1770443373558},{"file_id":"1k35_VL193DNC3IKQpq7AEKvlRW3XN8jQ","timestamp":1770442423445},{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1754723238928}],"collapsed_sections":["-Kee-DAl2viO","gIfDvo9L0UH2"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Project Name**    -  Tata Steel Machine Failure Prediction\n"],"metadata":{"id":"vncDsAP0Gaoa"}},{"cell_type":"markdown","source":["##### **Project Type**    - Classification\n","##### **Contribution**    - Individual\n"],"metadata":{"id":"beRrZCGUAJYm"}},{"cell_type":"markdown","source":["# **Project Summary -**"],"metadata":{"id":"FJNUwmbgGyua"}},{"cell_type":"markdown","source":["This project focuses on predicting machine failures in the steel manufacturing process at TATA Steel. In large-scale steel production, machines operate under heavy loads and extreme conditions, and unexpected breakdowns can lead to significant downtime, reduced product quality, and high maintenance costs. The aim is to develop a predictive system that can forecast these failures before they occur, enabling timely maintenance and improving overall operational efficiency. The dataset used contains various operational parameters such as air temperature, process temperature, rotational speed, torque, and tool wear, along with information on whether a machine failed and the type of failure. The data has been synthetically generated to reflect realistic industry patterns. The approach involves performing exploratory data analysis to understand the relationships between different parameters and machine failures, cleaning and preparing the data to ensure quality, and building classification models using algorithms like Random Forest, XGBoost, and LightGBM. The models are evaluated using accuracy, F1-score, and ROC-AUC, with special attention to handling class imbalance. Feature importance techniques are applied to understand which factors contribute most to failures. The final model aims to help TATA Steel reduce downtime, improve product quality, and minimize maintenance expenses through data-driven decision-making."],"metadata":{"id":"F6v_1wHtG2nS"}},{"cell_type":"markdown","source":["# **GitHub -**"],"metadata":{"id":"WdYSb5xYK45w"}},{"cell_type":"markdown","source":[],"metadata":{"id":"l4vtdj6El9ZP"}},{"cell_type":"markdown","source":["# **Problem Statement**\n"],"metadata":{"id":"yQaldy8SH6Dl"}},{"cell_type":"markdown","source":["The problem faced by TATA Steel is the occurrence of unexpected machine failures during the steel manufacturing process, which leads to costly downtime, reduced production efficiency, compromised product quality, and increased maintenance expenses. Machines in steel production operate under extreme conditions, making them prone to wear and failure if not monitored and maintained effectively. Currently, maintenance is often reactive, taking place only after a breakdown has occurred, which results in unplanned production halts. The objective of this project is to develop a machine learning–based predictive model that can accurately identify the likelihood of a machine failure before it happens, using operational data such as temperature, rotational speed, torque, and tool wear. By leveraging this predictive capability, TATA Steel can shift from reactive to proactive maintenance, scheduling repairs and part replacements in advance, thereby minimizing downtime, improving product quality, and optimizing operational efficiency. The solution should not only predict whether a failure will occur but also provide insights into the most critical factors influencing these failures, enabling better decision-making for maintenance planning."],"metadata":{"id":"DpeJGUA3kjGy"}},{"cell_type":"markdown","source":["# **General Guidelines** : -  "],"metadata":{"id":"mDgbUHAGgjLW"}},{"cell_type":"markdown","source":["1.   Well-structured, formatted, and commented code is required.\n","2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n","     \n","     The additional credits will have advantages over other students during Star Student selection.\n","       \n","             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n","                       without a single error logged. ]\n","\n","3.   Each and every logic should have proper comments.\n","4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n","        \n","\n","```\n","# Chart visualization code\n","```\n","            \n","\n","*   Why did you pick the specific chart?\n","*   What is/are the insight(s) found from the chart?\n","* Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason.\n","\n","5. You have to create at least 15 logical & meaningful charts having important insights.\n","\n","\n","[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n","\n","U - Univariate Analysis,\n","\n","B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n","\n","M - Multivariate Analysis\n"," ]\n","\n","\n","\n","\n","\n","6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n","\n","\n","*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n","\n","\n","*   Cross- Validation & Hyperparameter Tuning\n","\n","*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n","\n","*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZrxVaUj-hHfC"}},{"cell_type":"markdown","source":["# ***Let's Begin !***"],"metadata":{"id":"O_i_v8NEhb9l"}},{"cell_type":"markdown","source":["## ***1. Know Your Data***"],"metadata":{"id":"HhfV-JJviCcP"}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"Y3lxredqlCYt"}},{"cell_type":"code","source":["# Import Libraries\n","\n","# Data Manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Visualization\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","sns.set_style('darkgrid')\n","\n","\n","# Statistics\n","from scipy.stats import *\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","# Data Preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n","from imblearn.over_sampling import SMOTE\n","\n","# Models\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier, XGBRFClassifier\n","\n","# Evaluation Metrics\n","from sklearn.metrics import (\n","    accuracy_score, confusion_matrix, classification_report,\n","    roc_auc_score, roc_curve\n",")\n","\n","# Explainability\n","import shap\n","\n","# Misc\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"M8Vqi-pPk-HR","executionInfo":{"status":"ok","timestamp":1770450995428,"user_tz":-330,"elapsed":16012,"user":{"displayName":"vishal bhardwaj","userId":"07068598329163523661"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"3RnN4peoiCZX"}},{"cell_type":"code","source":["# Load Dataset\n","# File paths\n","train_path = \"/content/train.csv\"\n","test_path = \"/content/test.csv\"\n","\n","# Reading the CSV files\n","train_df = pd.read_csv(train_path)\n","test_df = pd.read_csv(test_path)\n","\n","# Quick check of the data\n","print(\"Training Data Shape:\", train_df.shape)\n","print(\"Testing Data Shape:\", test_df.shape)"],"metadata":{"id":"4CkvbW_SlZ_R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset First View"],"metadata":{"id":"x71ZqKXriCWQ"}},{"cell_type":"code","source":["# Dataset First Look\n","train_df.head()"],"metadata":{"id":"LWNFOSvLl09H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.head()"],"metadata":{"id":"0d9CH66zvaE2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Rows & Columns count"],"metadata":{"id":"7hBIi_osiCS2"}},{"cell_type":"code","source":["# Dataset Rows & Columns count\n","# Training dataset shape\n","print(f\"Training Dataset: {train_df.shape[0]} rows and {train_df.shape[1]} columns\")\n","\n","# Testing dataset shape\n","print(f\"Testing Dataset: {test_df.shape[0]} rows and {test_df.shape[1]} columns\")"],"metadata":{"id":"Kllu7SJgmLij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset Information"],"metadata":{"id":"JlHwYmJAmNHm"}},{"cell_type":"code","source":["# Dataset Info\n","# For training dataset\n","train_df.info()\n","\n","# For testing dataset\n","test_df.info()\n"],"metadata":{"id":"e9hRXRi6meOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Duplicate Values"],"metadata":{"id":"35m5QtbWiB9F"}},{"cell_type":"code","source":["# Dataset Duplicate Value Count\n","# Training dataset duplicates\n","train_duplicates = train_df.duplicated().sum()\n","print(f\"Duplicate rows in Training Dataset: {train_duplicates}\")\n","\n","# Testing dataset duplicates\n","test_duplicates = test_df.duplicated().sum()\n","print(f\"Duplicate rows in Testing Dataset: {test_duplicates}\")"],"metadata":{"id":"1sLdpKYkmox0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Missing Values/Null Values"],"metadata":{"id":"PoPl-ycgm1ru"}},{"cell_type":"code","source":["# Missing Values/Null Values Count\n","# Training dataset missing values\n","print(\"Missing Values in Training Dataset:\")\n","print(train_df.isnull().sum())\n","\n","print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Testing dataset missing values\n","print(\"Missing Values in Testing Dataset:\")\n","print(test_df.isnull().sum())"],"metadata":{"id":"GgHWkxvamxVg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualizing the missing values\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis')\n","plt.title(\"Missing Values Heatmap - Training Dataset\")\n","plt.show()\n","\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(test_df.isnull(), cbar=False, cmap='viridis')\n","plt.title(\"Missing Values Heatmap - Testing Dataset\")\n","plt.show()"],"metadata":{"id":"3q5wnI3om9sJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What did you know about your dataset?"],"metadata":{"id":"H0kj-8xxnORC"}},{"cell_type":"markdown","source":["The dataset provides details about machine operating conditions and recorded breakdowns from steel manufacturing processes at TATA Steel. It includes 14 columns, consisting of numerical variables such as air temperature, process temperature, rotational speed, torque, and tool wear, along with one categorical column (Type). There are also several binary indicators representing different failure categories (TWF, HDF, PWF, OSF, RNF). The primary target column, **Machine failure**, shows whether a particular machine experienced a failure.\n","\n","Both the training and testing datasets are clean, with no missing values or duplicate entries, so they can be used directly for analysis without additional preprocessing. This well-structured data makes it suitable for exploratory analysis to identify patterns between operating conditions and failures, which can support the development of predictive models for early detection of machine breakdowns.\n"],"metadata":{"id":"gfoNAAC-nUe_"}},{"cell_type":"markdown","source":["## ***2. Understanding Your Variables***"],"metadata":{"id":"nA9Y7ga8ng1Z"}},{"cell_type":"code","source":["# Training dataset columns\n","print(\"Training Dataset Columns:\")\n","print(train_df.columns.tolist())\n","\n","print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Testing dataset columns\n","print(\"Testing Dataset Columns:\")\n","print(test_df.columns.tolist())"],"metadata":{"id":"j7xfkqrt5Ag5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training Dataset - Statistical Summary:\")\n","display(train_df.describe().T)\n","\n","print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","print(\"Testing Dataset - Statistical Summary:\")\n","display(test_df.describe().T)"],"metadata":{"id":"DnOaZdaE5Q5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variables Description"],"metadata":{"id":"PBTbrJXOngz2"}},{"cell_type":"markdown","source":["The dataset is divided into two sections: a training set containing 136,429 records and a testing set with 90,954 records, both structured with the same set of columns. Each entry reflects machine sensor measurements and operating status during production, identified using an id and Product ID. Important numerical attributes include Air temperature [K], Process temperature [K], Rotational speed [rpm], Torque [Nm], and Tool wear [min], which describe the working conditions of the equipment.\n","\n","In the training data, the main target column, Machine failure, shows whether a breakdown occurred. It is further supported by five binary indicators that specify the type of failure: TWF (Tool Wear Failure), HDF (Heat Dissipation Failure), PWF (Power Failure), OSF (Overstrain Failure), and RNF (Random Failure). Together, these features make the dataset suitable for building predictive models that help detect patterns linked to failures and support preventive maintenance strategies.\n"],"metadata":{"id":"aJV4KIxSnxay"}},{"cell_type":"markdown","source":["### Check Unique Values for each variable."],"metadata":{"id":"u3PMJOP6ngxN"}},{"cell_type":"code","source":["# Unique value counts for training dataset\n","print(\"Unique Values in Training Dataset:\")\n","print(train_df.nunique())\n","\n","print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","# Unique value counts for testing dataset\n","print(\"Unique Values in Testing Dataset:\")\n","print(test_df.nunique())\n"],"metadata":{"id":"zms12Yq5n-jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. ***Data Wrangling***"],"metadata":{"id":"dauF4eBmngu3"}},{"cell_type":"markdown","source":["### Data Wrangling Code"],"metadata":{"id":"bKJF3rekwFvQ"}},{"cell_type":"code","source":["# Data Wrangling - Step 1: Copy original datasets\n","# ===========================================\n","\n","train_df_clean = train_df.copy()\n","test_df_clean = test_df.copy()\n"],"metadata":{"id":"wk-9a2fpoLcV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 2: Drop unnecessary columns\n","# ===========================================\n","# 'id' column is just an identifier and has no predictive power.\n","\n","train_df_clean.drop(columns=['id'], inplace=True)\n","test_df_clean.drop(columns=['id'], inplace=True)\n"],"metadata":{"id":"nQQAuI3dt2eS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 3: Standardize column names\n","# ===========================================\n","# Replace spaces and special characters with underscores for easier access.\n","\n","train_df_clean.columns = train_df_clean.columns.str.strip().str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n","test_df_clean.columns = test_df_clean.columns.str.strip().str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '', regex=True)\n"],"metadata":{"id":"XnnTlgFwt2aq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 4: Handle missing values\n","# ===========================================\n","# Our dataset currently has no missing values, but we add logic for robustness.\n","\n","for df in [train_df_clean, test_df_clean]:\n","    for col in df.columns:\n","        if df[col].isnull().sum() > 0:\n","            if df[col].dtype in ['float64', 'int64']:\n","                df[col].fillna(df[col].median(), inplace=True)  # Median for numerical columns\n","            else:\n","                df[col].fillna(df[col].mode()[0], inplace=True)  # Mode for categorical columns\n"],"metadata":{"id":"4bHx3cNzt2YT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 5: Encode categorical variables\n","# ===========================================\n","# 'Type' column is categorical; we use one-hot encoding.\n","\n","train_df_clean = pd.get_dummies(train_df_clean, columns=['Type'], drop_first=True)\n","test_df_clean = pd.get_dummies(test_df_clean, columns=['Type'], drop_first=True)\n","\n","# Ensure both datasets have the same columns after encoding\n","missing_cols = set(train_df_clean.columns) - set(test_df_clean.columns)\n","for col in missing_cols:\n","    test_df_clean[col] = 0  # Add missing columns in test set\n","\n","# Align column order\n","test_df_clean = test_df_clean[train_df_clean.columns.drop('Machine_failure')]\n"],"metadata":{"id":"zBJ-Ya_Ct2VT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 6: Remove duplicates\n","# ===========================================\n","\n","train_df_clean.drop_duplicates(inplace=True)\n","test_df_clean.drop_duplicates(inplace=True)\n"],"metadata":{"id":"kPwLufH5t2S5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 7: Reset index after cleaning\n","# ===========================================\n","\n","train_df_clean.reset_index(drop=True, inplace=True)\n","test_df_clean.reset_index(drop=True, inplace=True)\n"],"metadata":{"id":"Kat5WI-Ut2QS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================================\n","# Step 8: Final check after wrangling\n","# ===========================================\n","\n","print(\"Training Dataset Shape:\", train_df_clean.shape)\n","print(\"Testing Dataset Shape:\", test_df_clean.shape)\n","print(\"Training Columns:\", train_df_clean.columns.tolist())\n"],"metadata":{"id":"-xbJIBuVt2N0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### What all manipulations have you done and insights you found?"],"metadata":{"id":"MSa1f5Uengrz"}},{"cell_type":"markdown","source":["During the data wrangling phase, the Tata Steel Machine Failure dataset was refined to make it suitable for analysis and model development. Redundant identifier fields such as the `id` column were removed, and column names were normalized to improve readability and usability. Although the dataset did not contain missing values, a structured approach for handling them was incorporated to ensure robustness.\n","\n","The categorical feature `Type` was transformed using one-hot encoding, with careful alignment between the training and testing datasets so that both shared the same feature structure. Duplicate records were eliminated, and dataset indices were reset to maintain consistency. After these preprocessing steps, the training data comprised 135,295 rows across 14 columns, while the test data contained 90,431 rows and 13 columns, resulting in a clean, well-organized dataset ready for modeling and analysis.\n"],"metadata":{"id":"LbyXE7I1olp8"}},{"cell_type":"markdown","source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"],"metadata":{"id":"GF8Ens_Soomf"}},{"cell_type":"markdown","source":["#### Chart - 1"],"metadata":{"id":"0wOQAZs5pc--"}},{"cell_type":"code","source":["# ===========================================\n","# Chart 1: Distribution of Machine Failure (Bar Chart)\n","# ===========================================\n","\n","plt.figure(figsize=(6,5))\n","sns.countplot(x='Machine_failure', data=train_df_clean, palette=['lightgreen','tomato'])\n","plt.title(\"Machine Failure Distribution\")\n","plt.xticks([0,1], ['No Failure','Failure'])\n","plt.ylabel(\"Count\")\n","plt.show()\n"],"metadata":{"id":"7v_ESjsspbW7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"K5QZ13OEpz2H"}},{"cell_type":"markdown","source":["I chose a bar chart because it is the most effective way to visualize categorical target variable distribution. It clearly shows how many machines failed versus did not fail."],"metadata":{"id":"XESiWehPqBRc"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"lQ7QKXXCp7Bj"}},{"cell_type":"markdown","source":["The chart reveals a strong class imbalance, with the majority of cases showing no machine failure and very few failure cases. This highlights the rarity of machine breakdowns."],"metadata":{"id":"C_j1G7yiqdRP"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"448CDAPjqfQr"}},{"cell_type":"markdown","source":["Yes, this insight helps positively by showing the need for balanced modeling techniques (SMOTE, class weights, etc.) to avoid biased predictions. The imbalance itself is a risk because without correction, the model may learn to ignore failures, leading to missed opportunities for preventive maintenance."],"metadata":{"id":"3cspy4FjqxJW"}},{"cell_type":"markdown","source":["#### Chart - 2"],"metadata":{"id":"KSlN3yHqYklG"}},{"cell_type":"code","source":["# Chart 2: Air Temperature Distribution (Histogram)\n","\n","plt.figure(figsize=(8,6))\n","plt.hist(train_df_clean['Air_temperature_K'], bins=30, color='skyblue', edgecolor='black')\n","plt.title(\"Air Temperature Distribution\")\n","plt.xlabel(\"Air Temperature (K)\")\n","plt.ylabel(\"Frequency\")\n","plt.show()\n"],"metadata":{"id":"R4YgtaqtYklH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t6dVpIINYklI"}},{"cell_type":"markdown","source":["A histogram is the most suitable choice to analyze the distribution of continuous variables like air temperature. It clearly shows the frequency of observations across different ranges of temperatures."],"metadata":{"id":"5aaW0BYyYklI"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ijmpgYnKYklI"}},{"cell_type":"markdown","source":["Most of the air temperature values are concentrated around 298K–302K, with a peak near 300K. The distribution is slightly spread on both sides but remains within a narrow range, suggesting stable operating conditions."],"metadata":{"id":"PSx9atu2YklI"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"-JiQyfWJYklI"}},{"cell_type":"markdown","source":["Yes, the insights can create a positive business impact because knowing that machines operate within a stable temperature band helps in setting optimal thresholds for maintenance and failure detection. No negative growth is implied here, but deviations from this band in the future may indicate abnormal conditions, which should be monitored to prevent failures."],"metadata":{"id":"BcBbebzrYklV"}},{"cell_type":"markdown","source":["#### Chart - 3"],"metadata":{"id":"EM7whBJCYoAo"}},{"cell_type":"code","source":["# Chart 3: Boxplot of Process Temperature\n","plt.figure(figsize=(8,6))\n","sns.boxplot(y=train_df_clean['Process_temperature_K'], color=\"lightcoral\")\n","plt.title(\"Process Temperature Spread\")\n","plt.ylabel(\"Process Temperature (K)\")\n","plt.show()\n"],"metadata":{"id":"t6GMdE67YoAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"fge-S5ZAYoAp"}},{"cell_type":"markdown","source":["I used a boxplot because it clearly shows the median, interquartile range, and any potential outliers in process temperature, which helps in understanding variation and stability of machine operations."],"metadata":{"id":"5dBItgRVYoAp"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"85gYPyotYoAp"}},{"cell_type":"markdown","source":["The chart shows that most process temperatures lie between ~309K and ~311K, with a median near 310K. The distribution is fairly compact, and only a few mild outliers are visible, suggesting stable temperature control."],"metadata":{"id":"4jstXR6OYoAp"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"RoGjAbkUYoAp"}},{"cell_type":"markdown","source":["Yes, the insights are positive since controlled and consistent process temperature reduces machine stress and failure risk. However, even small deviations or outliers might cause operational inefficiencies, so monitoring those cases is important to prevent future breakdowns."],"metadata":{"id":"zfJ8IqMcYoAp"}},{"cell_type":"markdown","source":["#### Chart - 4"],"metadata":{"id":"4Of9eVA-YrdM"}},{"cell_type":"code","source":["# ===========================================\n","# Chart 4: Scatter Plot - Torque vs Rotational Speed\n","# ===========================================\n","\n","plt.figure(figsize=(8,6))\n","plt.scatter(train_df_clean['Rotational_speed_rpm'],\n","            train_df_clean['Torque_Nm'],\n","            alpha=0.3, c='teal')\n","\n","plt.title(\"Torque vs Rotational Speed\")\n","plt.xlabel(\"Rotational Speed (rpm)\")\n","plt.ylabel(\"Torque (Nm)\")\n","plt.show()\n"],"metadata":{"id":"irlUoxc8YrdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"iky9q4vBYrdO"}},{"cell_type":"markdown","source":["A scatter plot is ideal to analyze the relationship between two continuous variables. Torque and rotational speed directly influence machine performance, so plotting them helps identify patterns and operational clusters."],"metadata":{"id":"aJRCwT6DYrdO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"F6T5p64dYrdO"}},{"cell_type":"markdown","source":["The chart shows a clear inverse relationship: as rotational speed increases, torque tends to decrease. Most machines operate within specific ranges, forming dense clusters, while outliers represent unusual operations."],"metadata":{"id":"Xx8WAJvtYrdO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"y-Ehk30pYrdP"}},{"cell_type":"markdown","source":["Yes, the insights are valuable for optimizing machine operations. Running machines in stable torque-speed zones reduces stress and lowers failure risk, positively impacting productivity. However, operating outside these ranges (outliers) could lead to inefficiencies or potential failures, posing a risk of negative business impact."],"metadata":{"id":"jLNxxz7MYrdP"}},{"cell_type":"markdown","source":["#### Chart - 5"],"metadata":{"id":"bamQiAODYuh1"}},{"cell_type":"code","source":["# Chart 5: Failure Type Distribution (Pie Chart)\n","failure_types = ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n","failure_counts = train_df_clean[failure_types].sum()\n","\n","plt.figure(figsize=(7,7))\n","plt.pie(failure_counts, labels=failure_types, autopct='%1.1f%%', startangle=140, colors=plt.cm.Set3.colors)\n","plt.title(\"Distribution of Failure Types\", fontsize=14)\n","plt.show()\n"],"metadata":{"id":"TIJwrbroYuh3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"QHF8YVU7Yuh3"}},{"cell_type":"markdown","source":["I used a pie chart because it visually represents the proportion of each failure type, making it easy to compare their relative contribution to overall machine failures."],"metadata":{"id":"dcxuIMRPYuh3"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"GwzvFGzlYuh3"}},{"cell_type":"markdown","source":["Heat Dissipation Failure (HDF) is the most frequent issue (33.9%), followed by Overstrain Failure (OSF, 25.8%). Tool Wear Failure (TWF) contributes the least (10.1%). This shows which problems are most dominant in the system."],"metadata":{"id":"uyqkiB8YYuh3"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"qYpmQ266Yuh3"}},{"cell_type":"markdown","source":["Yes, these insights are highly useful because they allow Tata Steel to prioritize resources toward preventing HDF and OSF, which account for nearly 60% of failures. If ignored, these dominant failures could increase downtime and maintenance costs, negatively impacting production efficiency."],"metadata":{"id":"_WtzZ_hCYuh4"}},{"cell_type":"markdown","source":["#### Chart - 6"],"metadata":{"id":"OH-pJp9IphqM"}},{"cell_type":"code","source":["# ===========================================\n","# Chart 6: Tool Wear vs Machine Failure\n","# ===========================================\n","plt.figure(figsize=(8,6))\n","sns.boxplot(x='Machine_failure', y='Tool_wear_min', data=train_df_clean, palette=\"Set2\")\n","plt.title(\"Tool Wear vs Machine Failure\")\n","plt.xlabel(\"Machine Failure (0 = No, 1 = Yes)\")\n","plt.ylabel(\"Tool Wear (minutes)\")\n","plt.show()\n"],"metadata":{"id":"kuRf4wtuphqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"bbFf2-_FphqN"}},{"cell_type":"markdown","source":["I chose a box plot because it clearly shows the spread, median, and outliers of tool wear for both failing and non-failing machines. This helps compare distributions between the two groups."],"metadata":{"id":"loh7H2nzphqN"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"_ouA3fa0phqN"}},{"cell_type":"markdown","source":["Machines that fail generally have higher tool wear times compared to machines without failure. The median tool wear for failed machines is visibly greater, suggesting wear is a key driver of breakdowns."],"metadata":{"id":"VECbqPI7phqN"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"Seke61FWphqN"}},{"cell_type":"markdown","source":["Yes, the insight has a positive impact as it highlights the importance of monitoring tool wear proactively. By replacing or servicing tools before they reach high wear levels, machine failures can be reduced, minimizing downtime. No negative growth is implied since the insight directly suggests preventive maintenance benefits."],"metadata":{"id":"DW4_bGpfphqN"}},{"cell_type":"markdown","source":["#### Chart - 7"],"metadata":{"id":"PIIx-8_IphqN"}},{"cell_type":"code","source":["# ===========================================\n","# Chart 7: Rotational Speed vs Machine Failure\n","# ===========================================\n","plt.figure(figsize=(8,6))\n","sns.histplot(data=train_df_clean,\n","             x='Rotational_speed_rpm',\n","             hue='Machine_failure',\n","             bins=50,\n","             kde=False,\n","             palette={0:\"skyblue\",1:\"salmon\"},\n","             alpha=0.6)\n","\n","plt.title(\"Rotational Speed Distribution by Machine Failure\")\n","plt.xlabel(\"Rotational Speed (rpm)\")\n","plt.ylabel(\"Count\")\n","plt.legend(title=\"Failure\", labels=[\"No Failure\",\"Failure\"])\n","plt.show()\n"],"metadata":{"id":"lqAIGUfyphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"t27r6nlMphqO"}},{"cell_type":"markdown","source":["I chose a histogram with hue separation because it clearly shows how machine failures distribute across different rotational speeds, while also comparing them with non-failure cases."],"metadata":{"id":"iv6ro40sphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"r2jJGEOYphqO"}},{"cell_type":"markdown","source":["Most machines operate around 1400–1600 rpm, and failures are very rare across all ranges. Failures are slightly more noticeable at lower speeds, but overall, the majority of points are safe."],"metadata":{"id":"Po6ZPi4hphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"b0JNsNcRphqO"}},{"cell_type":"markdown","source":["Yes, knowing that failures don’t strongly depend on rotational speed helps operators focus monitoring efforts on other factors like torque and tool wear. No negative growth insight here—just validation that rpm is less critical."],"metadata":{"id":"xvSq8iUTphqO"}},{"cell_type":"markdown","source":["#### Chart - 8"],"metadata":{"id":"BZR9WyysphqO"}},{"cell_type":"code","source":["# Chart 8: Torque vs Machine Failure\n","plt.figure(figsize=(8,6))\n","sns.boxplot(x='Machine_failure', y='Torque_Nm', data=train_df_clean, palette=\"Set2\")\n","plt.title(\"Torque vs Machine Failure\")\n","plt.xlabel(\"Machine Failure (0 = No, 1 = Yes)\")\n","plt.ylabel(\"Torque (Nm)\")\n","plt.show()\n"],"metadata":{"id":"TdPTWpAVphqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"jj7wYXLtphqO"}},{"cell_type":"markdown","source":["A boxplot is effective for comparing torque distributions between failed and non-failed machines, highlighting medians, variability, and outliers."],"metadata":{"id":"Ob8u6rCTphqO"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"eZrbJ2SmphqO"}},{"cell_type":"markdown","source":["Machines that failed tend to operate at higher torque values on average compared to non-failed ones. This suggests that excessive torque may be a major contributor to machine breakdowns."],"metadata":{"id":"mZtgC_hjphqO"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"rFu4xreNphqO"}},{"cell_type":"markdown","source":["Yes, monitoring torque levels can help predict and prevent failures, reducing downtime and maintenance costs (positive impact). However, it also highlights that operating at high torque for extended periods accelerates wear and failure, meaning tighter production constraints may be required, which could slightly reduce throughput (short-term negative trade-off)."],"metadata":{"id":"ey_0qi68phqO"}},{"cell_type":"markdown","source":["#### Chart - 9"],"metadata":{"id":"YJ55k-q6phqO"}},{"cell_type":"code","source":["# Chart 9: Boxplot - Air Temperature vs Machine Failure\n","plt.figure(figsize=(8,6))\n","sns.boxplot(x='Machine_failure', y='Air_temperature_K', data=train_df_clean, palette=\"Set2\")\n","plt.title('Air Temperature vs Machine Failure')\n","plt.xlabel('Machine Failure (0 = No, 1 = Yes)')\n","plt.ylabel('Air Temperature (K)')\n","plt.show()\n"],"metadata":{"id":"B2aS4O1ophqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"gCFgpxoyphqP"}},{"cell_type":"markdown","source":["I chose a boxplot because it effectively compares the distribution of air temperature across two categories — machines with and without failures — highlighting median shifts and variability."],"metadata":{"id":"TVxDimi2phqP"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"OVtJsKN_phqQ"}},{"cell_type":"markdown","source":["Machines with failures tend to have slightly higher air temperatures compared to those without failures. The spread is similar, but the median is shifted upwards for failure cases."],"metadata":{"id":"ngGi97qjphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"lssrdh5qphqQ"}},{"cell_type":"markdown","source":["This is positive because it shows that air temperature monitoring could be an early indicator of potential failures. If ignored, high air temperature could negatively impact machine reliability and increase downtime."],"metadata":{"id":"tBpY5ekJphqQ"}},{"cell_type":"markdown","source":["#### Chart - 10"],"metadata":{"id":"U2RJ9gkRphqQ"}},{"cell_type":"code","source":["# Chart 10: Boxplot - Process Temperature vs Machine Failure\n","plt.figure(figsize=(8,6))\n","sns.boxplot(\n","    x='Machine_failure',\n","    y='Process_temperature_K',\n","    data=train_df_clean,\n","    palette=\"coolwarm\"\n",")\n","plt.title('Process Temperature vs Machine Failure')\n","plt.xlabel('Machine Failure (0 = No, 1 = Yes)')\n","plt.ylabel('Process Temperature (K)')\n","plt.show()\n"],"metadata":{"id":"GM7a4YP4phqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"1M8mcRywphqQ"}},{"cell_type":"markdown","source":["I used a boxplot since it effectively compares the distribution of process temperatures between failed and non-failed machines. It highlights medians, variability, and outliers clearly."],"metadata":{"id":"8agQvks0phqQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"tgIPom80phqQ"}},{"cell_type":"markdown","source":["Machines with failures tend to have slightly higher process temperatures compared to non-failures. Outliers are more visible in failure cases, indicating unstable operations."],"metadata":{"id":"Qp13pnNzphqQ"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"JMzcOPDDphqR"}},{"cell_type":"markdown","source":["Yes, monitoring process temperature can help detect early warning signs and reduce downtime. If ignored, overheating could increase failure rates, leading to production losses."],"metadata":{"id":"R4Ka1PC2phqR"}},{"cell_type":"markdown","source":["#### Chart - 11"],"metadata":{"id":"x-EpHcCOp1ci"}},{"cell_type":"code","source":["# Chart 11: Scatter Plot - Torque vs Rotational Speed colored by Machine Failure\n","plt.figure(figsize=(8,6))\n","sns.scatterplot(\n","    x='Rotational_speed_rpm',\n","    y='Torque_Nm',\n","    hue='Machine_failure',\n","    data=train_df_clean,\n","    palette=\"Set1\",\n","    alpha=0.6\n",")\n","plt.title('Torque vs Rotational Speed by Machine Failure')\n","plt.xlabel('Rotational Speed (rpm)')\n","plt.ylabel('Torque (Nm)')\n","plt.legend(title=\"Machine Failure\", labels=[\"No Failure\", \"Failure\"])\n","plt.show()\n"],"metadata":{"id":"mAQTIvtqp1cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"X_VqEhTip1ck"}},{"cell_type":"markdown","source":["A scatter plot is best to analyze the relationship between two continuous variables (torque and speed) while showing how failures are distributed. It helps visualize operational zones and anomalies."],"metadata":{"id":"-vsMzt_np1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"8zGJKyg5p1ck"}},{"cell_type":"markdown","source":["There is a clear inverse relationship: torque decreases as rotational speed increases. Failures appear more concentrated at high torque–low speed zones."],"metadata":{"id":"ZYdMsrqVp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"PVzmfK_Ep1ck"}},{"cell_type":"markdown","source":["Yes, this insight can guide operators to avoid high-torque, low-speed operations to reduce failures. If ignored, such patterns can increase downtime and negatively affect productivity."],"metadata":{"id":"druuKYZpp1ck"}},{"cell_type":"markdown","source":["#### Chart - 12"],"metadata":{"id":"n3dbpmDWp1ck"}},{"cell_type":"code","source":["# Chart 12: Histogram - Tool Wear Distribution by Machine Failure\n","plt.figure(figsize=(8,6))\n","sns.histplot(\n","    data=train_df_clean,\n","    x='Tool_wear_min',\n","    hue='Machine_failure',\n","    kde=True,\n","    bins=40,\n","    palette=\"Dark2\",\n","    alpha=0.6\n",")\n","plt.title('Tool Wear Distribution by Machine Failure')\n","plt.xlabel('Tool Wear (minutes)')\n","plt.ylabel('Count')\n","plt.legend(title=\"Machine Failure\", labels=[\"No Failure\", \"Failure\"])\n","plt.show()\n"],"metadata":{"id":"bwevp1tKp1ck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"ylSl6qgtp1ck"}},{"cell_type":"markdown","source":["A histogram with KDE is suitable for visualizing how tool wear values are distributed and how failures differ from non-failures across ranges."],"metadata":{"id":"m2xqNkiQp1ck"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"ZWILFDl5p1ck"}},{"cell_type":"markdown","source":["Failures are more frequent at higher tool wear values, while non-failure machines dominate at lower wear levels. Tool wear shows a clear correlation with breakdowns."],"metadata":{"id":"x-lUsV2mp1ck"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"M7G43BXep1ck"}},{"cell_type":"markdown","source":["Yes, this emphasizes scheduling preventive maintenance before tools reach high wear levels. Ignoring this trend risks higher failure rates and costly downtime."],"metadata":{"id":"5wwDJXsLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 13"],"metadata":{"id":"Ag9LCva-p1cl"}},{"cell_type":"code","source":["# Check all column names in dataset\n","print(train_df.columns.tolist())\n"],"metadata":{"id":"4mF-UCBvyHL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chart 13: Scatter Plot - Process Temperature vs Air Temperature (colored by Machine Failure)\n","plt.figure(figsize=(8,6))\n","sns.scatterplot(\n","    x='Air_temperature_K',\n","    y='Process_temperature_K',\n","    hue='Machine_failure',\n","    data=train_df_clean,\n","    alpha=0.6,\n","    palette='Set1'\n",")\n","plt.title('Process Temperature vs Air Temperature by Machine Failure')\n","plt.xlabel('Air Temperature (K)')\n","plt.ylabel('Process Temperature (K)')\n","plt.legend(title='Machine Failure')\n","plt.show()\n"],"metadata":{"id":"EUfxeq9-p1cl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"E6MkPsBcp1cl"}},{"cell_type":"markdown","source":["I chose a scatter plot because it’s the best way to show how two continuous variables (air and process temperature) are related and how failures appear in that relationship."],"metadata":{"id":"V22bRsFWp1cl"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"2cELzS2fp1cl"}},{"cell_type":"markdown","source":["There is a strong positive relation between air temperature and process temperature. Failures are scattered but mostly follow the same trend as non-failures."],"metadata":{"id":"ozQPc2_Ip1cl"}},{"cell_type":"markdown","source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."],"metadata":{"id":"3MPXvC8up1cl"}},{"cell_type":"markdown","source":["Yes, because monitoring both temperatures together can help detect abnormal points. If ignored, unusual temperature combinations could lead to machine breakdowns."],"metadata":{"id":"GL8l1tdLp1cl"}},{"cell_type":"markdown","source":["#### Chart - 14 - Correlation Heatmap"],"metadata":{"id":"NC_X3p0fY2L0"}},{"cell_type":"code","source":["# Chart 14: Correlation Heatmap of Features (Numeric Only)\n","plt.figure(figsize=(12,8))\n","\n","# Select only numeric columns\n","numeric_df = train_df_clean.select_dtypes(include=['int64','float64'])\n","\n","# Compute correlation\n","corr = numeric_df.corr()\n","\n","# Plot heatmap\n","sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, square=True)\n","plt.title('Correlation Heatmap of Machine Failure Features')\n","plt.show()\n"],"metadata":{"id":"xyC9zolEZNRQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"UV0SzAkaZNRQ"}},{"cell_type":"markdown","source":["I chose a heatmap because it makes it easy to see relationships between many numeric features at once, showing both strong and weak correlations."],"metadata":{"id":"DVPuT8LYZNRQ"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"YPEH6qLeZNRQ"}},{"cell_type":"markdown","source":["Air temperature and process temperature have a very strong positive correlation.\n","Rotational speed and torque have a strong negative correlation (when speed goes up, torque goes down).\n","Machine failure is more related to tool wear, HDF (Heat Dissipation Failure), and OSF (Overstrain Failure)."],"metadata":{"id":"bfSqtnDqZNRR"}},{"cell_type":"markdown","source":["#### Chart - 15 - Pair Plot"],"metadata":{"id":"q29F0dvdveiT"}},{"cell_type":"code","source":["# Chart 15: Pair Plot (Sampled for Efficiency)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Select features for pair plot\n","pairplot_features = [\n","    'Air_temperature_K',\n","    'Process_temperature_K',\n","    'Rotational_speed_rpm',\n","    'Torque_Nm',\n","    'Tool_wear_min',\n","    'Machine_failure'\n","]\n","\n","# Take a random sample of 5000 rows for faster plotting\n","sample_df = train_df_clean.sample(5000, random_state=42)\n","\n","# Create pair plot\n","sns.pairplot(sample_df[pairplot_features], hue=\"Machine_failure\", diag_kind=\"kde\", palette=\"Set1\")\n","plt.suptitle(\"Pair Plot of Key Features vs Machine Failure (Sampled 5000 Rows)\", y=1.02)\n","plt.show()\n"],"metadata":{"id":"o58-TEIhveiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### 1. Why did you pick the specific chart?"],"metadata":{"id":"EXh0U9oCveiU"}},{"cell_type":"markdown","source":["I chose a pair plot because it allows us to see relationships between multiple features at the same time, both individually (distributions) and jointly (scatter plots). It helps to visually check patterns that might explain machine failures."],"metadata":{"id":"eMmPjTByveiU"}},{"cell_type":"markdown","source":["##### 2. What is/are the insight(s) found from the chart?"],"metadata":{"id":"22aHeOlLveiV"}},{"cell_type":"markdown","source":["Air temperature and process temperature are highly correlated (move together).\n","Rotational speed and torque show a strong negative relation.\n","Failures (blue points) are rare compared to non-failures (red points), but they are scattered across different feature ranges instead of being grouped in one region."],"metadata":{"id":"uPQ8RGwHveiV"}},{"cell_type":"markdown","source":["## ***5. Hypothesis Testing***"],"metadata":{"id":"g-ATYxFrGrvw"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 1"],"metadata":{"id":"8yEUt7NnHlrM"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"tEA2Xm5dHt1r"}},{"cell_type":"markdown","source":["\n","\n","*   Null Hypothesis (H₀): There is no significant difference in the average torque between machines that failed and machines that did not fail.\n","*   Alternate Hypothesis (H₁): There is a significant difference in the average torque between machines that failed and machines that did not fail.\n"],"metadata":{"id":"HI9ZP0laH0D-"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"I79__PHVH19G"}},{"cell_type":"code","source":["# Perform Statistical Test to obtain P-Value\n","from scipy.stats import ttest_ind\n","\n","# Separate torque values based on machine failure\n","torque_failure = train_df_clean[train_df_clean['Machine_failure'] == 1]['Torque_Nm']\n","torque_no_failure = train_df_clean[train_df_clean['Machine_failure'] == 0]['Torque_Nm']\n","\n","# Perform independent t-test\n","t_stat, p_val = ttest_ind(torque_failure, torque_no_failure, equal_var=False)  # Welch's t-test\n","print(\"T-statistic:\", t_stat)\n","print(\"P-value:\", p_val)\n"],"metadata":{"id":"oZrfquKtyian"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"Ou-I18pAyIpj"}},{"cell_type":"markdown","source":["I performed the Independent Samples t-test (Welch’s t-test) to compare the mean torque between machines that failed and machines that did not fail."],"metadata":{"id":"s2U0kk00ygSB"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"fF3858GYyt-u"}},{"cell_type":"markdown","source":["We are comparing the means of a continuous variable (Torque) across two independent groups (Failure vs No Failure).\n","\n","The assumption of equal variances may not hold in real-world manufacturing data, and Welch’s t-test is more robust since it does not assume equal variances.\n","\n","It provides a reliable way to determine if there is a statistically significant difference in torque between failed and non-failed machines."],"metadata":{"id":"HO4K0gP5y3B4"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 2"],"metadata":{"id":"4_0_7-oCpUZd"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"hwyV_J3ipUZe"}},{"cell_type":"markdown","source":["Null Hypothesis (H₀): There is no significant difference in average process temperature between machines that fail and those that do not fail.\n","\n","Alternative Hypothesis (H₁): There is a significant difference in average process temperature between failed and non-failed machines."],"metadata":{"id":"FnpLGJ-4pUZe"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"3yB-zSqbpUZe"}},{"cell_type":"code","source":["# Hypothesis Testing - Statement 2\n","# Compare Process Temperature between Machine Failure vs No Failure\n","\n","from scipy.stats import ttest_ind\n","\n","# Split the data into two groups\n","failures_temp = train_df_clean[train_df_clean['Machine_failure'] == 1]['Process_temperature_K']\n","no_failures_temp = train_df_clean[train_df_clean['Machine_failure'] == 0]['Process_temperature_K']\n","\n","# Perform Welch's t-test (equal_var=False)\n","t_stat_temp, p_val_temp = ttest_ind(failures_temp, no_failures_temp, equal_var=False)\n","\n","print(\"T-statistic:\", t_stat_temp)\n","print(\"P-value:\", p_val_temp)\n"],"metadata":{"id":"sWxdNTXNpUZe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"dEUvejAfpUZe"}},{"cell_type":"markdown","source":["I performed an Independent Samples t-test (Welch’s t-test) to compare the mean process temperature between failed and non-failed machines."],"metadata":{"id":"oLDrPz7HpUZf"}},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"Fd15vwWVpUZf"}},{"cell_type":"markdown","source":["This test was chosen because process temperature is a continuous variable, and we are comparing it across two independent groups (failure vs no failure). Welch’s t-test is more reliable than the standard t-test when group variances may not be equal, making it the most appropriate method here."],"metadata":{"id":"4xOGYyiBpUZf"}},{"cell_type":"markdown","source":["### Hypothetical Statement - 3"],"metadata":{"id":"bn_IUdTipZyH"}},{"cell_type":"markdown","source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."],"metadata":{"id":"49K5P_iCpZyH"}},{"cell_type":"markdown","source":["Null Hypothesis (H₀): Machine failure is independent of product type.\n","\n","Alternate Hypothesis (H₁): Machine failure depends on product type."],"metadata":{"id":"7gWI5rT9pZyH"}},{"cell_type":"markdown","source":["#### 2. Perform an appropriate statistical test."],"metadata":{"id":"Nff-vKELpZyI"}},{"cell_type":"code","source":["# Recreate Product Type column from one-hot encoded Type_L and Type_M\n","def get_product_type(row):\n","    if row['Type_L'] == 1:\n","        return 'L'\n","    elif row['Type_M'] == 1:\n","        return 'M'\n","    else:\n","        return 'H'   # If not L or M, then H\n","\n","train_df_clean['Product_Type'] = train_df_clean.apply(get_product_type, axis=1)\n","\n","# Create contingency table\n","contingency_table = pd.crosstab(train_df_clean['Product_Type'], train_df_clean['Machine_failure'])\n","\n","print(\"Contingency Table:\")\n","print(contingency_table)\n","\n","# Perform Chi-square test\n","from scipy.stats import chi2_contingency\n","chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n","\n","print(\"\\nChi-Square Test Results:\")\n","print(f\"Chi2 Statistic: {chi2:.4f}\")\n","print(f\"Degrees of Freedom: {dof}\")\n","print(f\"P-Value: {p_value:.4e}\")\n","\n","# Interpretation\n","if p_value < 0.05:\n","    print(\"\\nConclusion: Reject the null hypothesis.\")\n","    print(\"There is a significant relationship between Product Type and Machine Failure.\")\n","else:\n","    print(\"\\nConclusion: Fail to reject the null hypothesis.\")\n","    print(\"No significant relationship between Product Type and Machine Failure.\")\n"],"metadata":{"id":"s6AnJQjtpZyI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Why did you choose the specific statistical test?"],"metadata":{"id":"dWbDXHzopZyI"}},{"cell_type":"markdown","source":["I used the Chi-Square Test of Independence to calculate the p-value."],"metadata":{"id":"awWXg1VTR1ml"}},{"cell_type":"markdown","source":["##### Which statistical test have you done to obtain P-Value?"],"metadata":{"id":"kLW572S8pZyI"}},{"cell_type":"markdown","source":["The Chi-Square test is best suited for testing the relationship between two categorical variables (here, Product Type and Machine Failure). It helps check whether product type significantly influences the likelihood of machine failure."],"metadata":{"id":"M99G98V6pZyI"}},{"cell_type":"markdown","source":["## ***6. Feature Engineering & Data Pre-processing***"],"metadata":{"id":"yLjJCtPM0KBk"}},{"cell_type":"markdown","source":["### 1. Handling Missing Values"],"metadata":{"id":"xiyOF9F70UgQ"}},{"cell_type":"code","source":["# Check for missing values\n","train_df_clean.isnull().sum()\n","\n","# Handle missing values\n","train_df_clean = train_df_clean.fillna(train_df_clean.median(numeric_only=True))\n"],"metadata":{"id":"iRsAHk1K0fpS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"],"metadata":{"id":"7wuGOrhz0itI"}},{"cell_type":"markdown","source":["I used median imputation for missing numeric values because it is robust against outliers and keeps the data’s central tendency.\n","Since categorical columns didn’t have missing values, no imputation was needed there."],"metadata":{"id":"1ixusLtI0pqI"}},{"cell_type":"markdown","source":["### 2. Handling Outliers"],"metadata":{"id":"id1riN9m0vUs"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Select only numeric columns\n","numeric_cols = train_df_clean.select_dtypes(include=['int64', 'float64']).columns\n","\n","# Calculate IQR\n","Q1 = train_df_clean[numeric_cols].quantile(0.25)\n","Q3 = train_df_clean[numeric_cols].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define filtering condition (keep rows within IQR range)\n","condition = ~((train_df_clean[numeric_cols] < (Q1 - 1.5 * IQR)) |\n","              (train_df_clean[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)\n","\n","# Apply condition to remove outliers\n","train_df_clean = train_df_clean[condition]\n","\n","print(\"✅ Outliers handled successfully!\")\n","print(f\"Remaining rows after outlier removal: {train_df_clean.shape[0]}\")"],"metadata":{"id":"M6w2CzZf04JK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"],"metadata":{"id":"578E2V7j08f6"}},{"cell_type":"markdown","source":["I used the Interquartile Range (IQR) method to detect and remove outliers.\n","This method identifies values that lie far below the first quartile (Q1) or far above the third quartile (Q3).\n","It is a simple and effective technique because it works well for continuous numerical features and ensures that extreme values do not distort the model’s performance or statistical analysis."],"metadata":{"id":"uGZz5OrT1HH-"}},{"cell_type":"markdown","source":["### 3. Categorical Encoding"],"metadata":{"id":"89xtkJwZ18nB"}},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Check which columns are categorical\n","categorical_cols = train_df_clean.select_dtypes(include=['object']).columns\n","print(\"Categorical Columns:\", list(categorical_cols))\n","\n","# Apply Label Encoding to categorical columns\n","le = LabelEncoder()\n","for col in categorical_cols:\n","    train_df_clean[col] = le.fit_transform(train_df_clean[col])\n","\n","print(\"✅ Categorical encoding completed successfully!\")\n","print(train_df_clean.head())\n"],"metadata":{"id":"21JmIYMG2hEo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"],"metadata":{"id":"67NQN5KX2AMe"}},{"cell_type":"markdown","source":["I used Label Encoding and One-Hot Encoding for the categorical columns.\n","Label Encoding was applied to the Product_Type column to convert categories into numeric form for model compatibility.\n","One-Hot Encoding was used for the Type column (Type_L, Type_M) to avoid introducing ordinal relationships between categories.\n","These techniques ensure that categorical data is properly represented for machine learning algorithms without biasing the model."],"metadata":{"id":"UDaue5h32n_G"}},{"cell_type":"markdown","source":["### 4. Feature Manipulation & Selection"],"metadata":{"id":"-oLEiFgy-5Pf"}},{"cell_type":"markdown","source":["#### 1. Feature Manipulation"],"metadata":{"id":"C74aWNz2AliB"}},{"cell_type":"code","source":["# Manipulate Features to minimize feature correlation and create new features\n","train_df_clean['Temp_Diff'] = train_df_clean['Process_temperature_K'] - train_df_clean['Air_temperature_K']\n","print(\"✅ Created new feature: Temperature Difference (Process - Air)\")\n"],"metadata":{"id":"h1qC4yhBApWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Feature Selection"],"metadata":{"id":"2DejudWSA-a0"}},{"cell_type":"code","source":["# Select important features to avoid overfitting\n","corr = train_df_clean.corr()\n","high_corr = corr['Machine_failure'].abs().sort_values(ascending=False)\n","print(high_corr.head(10))\n"],"metadata":{"id":"YLhe8UmaBCEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What all feature selection methods have you used  and why?"],"metadata":{"id":"pEMng2IbBLp7"}},{"cell_type":"markdown","source":["*  Correlation Analysis – to identify the strongest predictors of failure.\n","*  Domain Knowledge – to retain features that have operational relevance.\n","\n"],"metadata":{"id":"rb2Lh6Z8BgGs"}},{"cell_type":"markdown","source":["##### Which all features you found important and why?"],"metadata":{"id":"rAdphbQ9Bhjc"}},{"cell_type":"markdown","source":["* Torque_Nm: Directly affects machine stress and failures.\n","* Tool_wear_min: Higher wear indicates higher failure risk.\n","* Rotational_speed_rpm: Affects torque and temperature.\n","* Process_temperature_K: Helps detect overheating.\n","* Product_Type: Different types have varying failure behavior."],"metadata":{"id":"fGgaEstsBnaf"}},{"cell_type":"markdown","source":["### 5. Data Transformation"],"metadata":{"id":"TNVZ9zx19K6k"}},{"cell_type":"markdown","source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"],"metadata":{"id":"nqoHp30x9hH9"}},{"cell_type":"code","source":["# Transform your data (if required)\n","# Applying log transformation to reduce skewness for Torque\n","train_df_clean['Torque_Nm_log'] = np.log1p(train_df_clean['Torque_Nm'])\n","print(\"✅ Applied log transformation on Torque_Nm to reduce skewness.\")\n"],"metadata":{"id":"I6quWQ1T9rtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data transformation was applied to normalize skewed features.\n","Log transformation reduces the effect of extreme torque values, improving model stability and performance."],"metadata":{"id":"8MduKk-5Y4bt"}},{"cell_type":"markdown","source":["### 6. Data Scaling"],"metadata":{"id":"rMDnDkt2B6du"}},{"cell_type":"code","source":["# Scaling your data using StandardScaler\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaled_features = scaler.fit_transform(train_df_clean[['Air_temperature_K', 'Process_temperature_K', 'Rotational_speed_rpm', 'Torque_Nm', 'Tool_wear_min']])\n","print(\"✅ Scaling completed using StandardScaler.\")\n"],"metadata":{"id":"dL9LWpySC6x_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which method have you used to scale you data and why?"],"metadata":{"id":"yiiVWRdJDDil"}},{"cell_type":"markdown","source":["Method Used: StandardScaler\n","It standardizes numerical features to a mean of 0 and standard deviation of 1.\n","Scaling ensures that all features contribute equally during model training."],"metadata":{"id":"T6y_k9VgY_nr"}},{"cell_type":"markdown","source":["### 7. Dimesionality Reduction"],"metadata":{"id":"1UUpS68QDMuG"}},{"cell_type":"markdown","source":["##### Do you think that dimensionality reduction is needed? Explain Why?"],"metadata":{"id":"kexQrXU-DjzY"}},{"cell_type":"markdown","source":["Dimensionality reduction was not required for this dataset.\n","The number of features is manageable and all carry meaningful information.\n","Hence, techniques like PCA were not applied."],"metadata":{"id":"GGRlBsSGDtTQ"}},{"cell_type":"markdown","source":["### 8. Data Splitting"],"metadata":{"id":"BhH2vgX9EjGr"}},{"cell_type":"code","source":["# Split data into train and test sets\n","from sklearn.model_selection import train_test_split\n","\n","X = train_df_clean.drop('Machine_failure', axis=1)\n","y = train_df_clean['Machine_failure']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","print(\"✅ Data successfully split into 80% training and 20% testing sets.\")\n"],"metadata":{"id":"0CTyd2UwEyNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### What data splitting ratio have you used and why?"],"metadata":{"id":"qjKvONjwE8ra"}},{"cell_type":"markdown","source":["Splitting Ratio: 80:20\n","The 80–20 split ensures sufficient data for training while keeping a fair portion for unbiased testing."],"metadata":{"id":"Y2lJ8cobFDb_"}},{"cell_type":"markdown","source":["### 9. Handling Imbalanced Dataset"],"metadata":{"id":"P1XJ9OREExlT"}},{"cell_type":"markdown","source":["##### Do you think the dataset is imbalanced? Explain Why."],"metadata":{"id":"VFOzZv6IFROw"}},{"cell_type":"markdown","source":["The dataset contains only one class (0 = No Failure), so there are no examples of failures (1). This means the dataset is not suitable for imbalance correction because there is no second class to balance."],"metadata":{"id":"GeKDIv7pFgcC"}},{"cell_type":"markdown","source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"],"metadata":{"id":"TIqpNgepFxVj"}},{"cell_type":"markdown","source":["Since the dataset contains only one class, I did not apply any balancing technique like SMOTE. Such techniques require at least two classes. Therefore, I skipped this step and continued with the available data."],"metadata":{"id":"qbet1HwdGDTz"}},{"cell_type":"markdown","source":["## ***7. ML Model Implementation***"],"metadata":{"id":"VfCC591jGiD4"}},{"cell_type":"markdown","source":["### ML Model - 1"],"metadata":{"id":"OB4l2ZhMeS1U"}},{"cell_type":"code","source":["# Step 1: check target in cleaned dataset\n","print(\"train_df_clean shape:\", train_df_clean.shape)\n","print(train_df_clean['Machine_failure'].value_counts(dropna=False))\n"],"metadata":{"id":"gh6Uc1eFui2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2: check target in original raw dataset (before cleaning/outlier removal)\n","print(\"Original train_df shape:\", train_df.shape)\n","\n","# Use whichever column name matches your original data\n","if 'Machine_failure' in train_df.columns:\n","    print(train_df['Machine_failure'].value_counts(dropna=False))\n","elif 'Machine failure' in train_df.columns:\n","    print(train_df['Machine failure'].value_counts(dropna=False))\n","else:\n","    print(\"Target column not found! Columns are:\", train_df.columns.tolist())\n"],"metadata":{"id":"6totVoCEufTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Restore missing failure rows into cleaned data\n","# (Combine failure rows from original train_df with your cleaned data)\n","\n","# Identify the failure rows from original data\n","failure_rows = train_df[train_df['Machine failure'] == 1]\n","\n","# Rename column to match your cleaned dataframe naming\n","failure_rows = failure_rows.rename(columns={'Machine failure': 'Machine_failure'})\n","\n","# Combine with cleaned dataset (avoid duplicates)\n","train_df_final = pd.concat([train_df_clean, failure_rows], ignore_index=True).drop_duplicates()\n","\n","print(\"✅ Combined dataset created successfully!\")\n","print(\"New shape:\", train_df_final.shape)\n","print(\"\\nNew class distribution:\")\n","print(train_df_final['Machine_failure'].value_counts())\n"],"metadata":{"id":"Sa6bQlNguJXq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Prepare data for Logistic Regression\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Features and target\n","X = train_df_final.drop(columns=['Machine_failure'])\n","y = train_df_final['Machine_failure']\n","\n","# Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, stratify=y, random_state=42\n",")\n","\n","print(\"✅ Stratified Train-Test Split Done!\")\n","print(\"\\nTraining set class distribution:\\n\", y_train.value_counts())\n","print(\"\\nTesting set class distribution:\\n\", y_test.value_counts())\n","\n","# Scale numerical columns\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train.select_dtypes(include=['float64', 'int64']))\n","X_test_scaled = scaler.transform(X_test.select_dtypes(include=['float64', 'int64']))\n","\n","print(\"\\n✅ Data Scaling Completed Successfully!\")\n"],"metadata":{"id":"yxB0HUQNunmJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Step 4.1: Handle Missing Values Before Training\n","\n","print(\"Missing values before imputation:\")\n","print(\"Train:\", np.isnan(X_train_scaled).sum())\n","print(\"Test :\", np.isnan(X_test_scaled).sum())\n","\n","# Replace NaN values with column means\n","# Compute column means ignoring NaN values\n","col_means_train = np.nanmean(X_train_scaled, axis=0)\n","col_means_test = np.nanmean(X_test_scaled, axis=0)\n","\n","# Find indices where NaNs exist and replace with column mean\n","inds_train = np.where(np.isnan(X_train_scaled))\n","inds_test = np.where(np.isnan(X_test_scaled))\n","\n","X_train_scaled[inds_train] = np.take(col_means_train, inds_train[1])\n","X_test_scaled[inds_test] = np.take(col_means_test, inds_test[1])\n","\n","print(\"\\n✅ Missing values handled successfully!\")\n","print(\"Remaining NaNs in train:\", np.isnan(X_train_scaled).sum())\n","print(\"Remaining NaNs in test :\", np.isnan(X_test_scaled).sum())\n"],"metadata":{"id":"m-XQ0qksu7aN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Logistic Regression Model Training & Evaluation\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Initialize and train model\n","log_reg = LogisticRegression(max_iter=1000, random_state=42)\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Predict on test set\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Evaluation metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","print(\"✅ Logistic Regression Model Trained Successfully!\\n\")\n","print(f\"Accuracy:  {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall:    {recall:.4f}\")\n","print(f\"F1 Score:  {f1:.4f}\\n\")\n","\n","# Classification report\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","\n","# Confusion Matrix\n","plt.figure(figsize=(6,4))\n","sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n","plt.title(\"Confusion Matrix - Logistic Regression\")\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n"],"metadata":{"id":"Zubz6tjNfo_N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"ArJBuiUVfxKd"}},{"cell_type":"markdown","source":["Model Explanation:\n","\n","* Algorithm Used: Logistic Regression\n","* Reason for Choosing:\n","  \n","  Logistic Regression is simple, interpretable, and effective for binary classification (in this case, predicting Machine Failure = 0 or 1).\n","  It works well when the relationship between independent variables and the target variable is approximately linear."],"metadata":{"id":"AFNWamnqvVc9"}},{"cell_type":"markdown","source":["Performance Metrics:\n","* Metric\tScore\n","* Accuracy\t0.9967\n","* Precision\t1.0000\n","* Recall\t0.8023\n","* F1 Score\t0.8903"],"metadata":{"id":"k1rx3Jm1vv-e"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n","values = [0.9967, 1.0000, 0.8023, 0.8903]\n","\n","plt.bar(metrics, values)\n","plt.title(\"Logistic Regression Performance Metrics\")\n","plt.ylim(0, 1.1)\n","plt.show()\n"],"metadata":{"id":"rqD5ZohzfxKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluation Metric Explanation:\n","* Accuracy: Model predicted correctly 99.67% of the time.\n","* Precision: All machine failures predicted by the model were correct (no false alarms).\n","* Recall: Model correctly detected 80.23% of the actual machine failures (a few missed failures).\n","* F1 Score: Balanced measure of precision and recall, at 0.89, showing excellent model reliability."],"metadata":{"id":"3I-9s40lvpRQ"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"4qY1EAkEfxKe"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","\n","# Define parameter grid\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","    'solver': ['liblinear', 'lbfgs'],\n","    'penalty': ['l2']\n","}\n","\n","# Initialize model\n","log_reg = LogisticRegression(max_iter=1000, random_state=42)\n","\n","# Grid Search with 5-fold Cross Validation\n","grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid,\n","                           cv=5, scoring='f1', n_jobs=-1, verbose=1)\n","\n","grid_search.fit(X_train_scaled, y_train)\n","\n","print(\"✅ Grid Search Completed Successfully!\")\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best F1 Score from CV:\", grid_search.best_score_)\n"],"metadata":{"id":"Dy61ujd6fxKe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrain model with best parameters\n","best_log_reg = grid_search.best_estimator_\n","best_log_reg.fit(X_train_scaled, y_train)\n","\n","# Predictions\n","y_pred_best = best_log_reg.predict(X_test_scaled)\n","\n","# Evaluate performance\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","acc_best = accuracy_score(y_test, y_pred_best)\n","prec_best = precision_score(y_test, y_pred_best)\n","rec_best = recall_score(y_test, y_pred_best)\n","f1_best = f1_score(y_test, y_pred_best)\n","\n","print(\"✅ Final Model Performance After Tuning:\")\n","print(f\"Accuracy:  {acc_best:.4f}\")\n","print(f\"Precision: {prec_best:.4f}\")\n","print(f\"Recall:    {rec_best:.4f}\")\n","print(f\"F1 Score:  {f1_best:.4f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n"],"metadata":{"id":"izo2vZ_RxY9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"PiV4Ypx8fxKe"}},{"cell_type":"markdown","source":["I used GridSearchCV with 5-fold cross-validation. It tests different parameter combinations and selects the best ones based on performance. I tuned parameters like C, penalty, and solver to improve the Logistic Regression model and prevent overfitting."],"metadata":{"id":"negyGRa7fxKf"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"TfvqoZmBfxKf"}},{"cell_type":"markdown","source":["Have you seen any improvement?\n","\n","After tuning, the model performed consistently well.\n","\n","* Metric\t  Before\t  After\n","* Accuracy   0.9967 0.9967\n","* Precision\t1.0000\t 1.0000\n","* Recall\t0.8023\t 0.8023\n","* F1 Score\t0.8903\t0.8903\n","\n","The results show the model is already optimized, giving high accuracy and perfect precision.\n","This helps in predicting machine failures early and reducing maintenance costs."],"metadata":{"id":"OaLui8CcfxKf"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","before = [0.9967, 1.0000, 0.8023, 0.8903]\n","after = [0.9971, 1.0000, 0.8255, 0.9044]\n","labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n","\n","x = range(len(labels))\n","plt.bar(x, before, width=0.4, label='Before Tuning', align='center')\n","plt.bar([i + 0.4 for i in x], after, width=0.4, label='After Tuning', align='center')\n","\n","plt.xticks([i + 0.2 for i in x], labels)\n","plt.ylim(0.75, 1.05)\n","plt.title(\"Model Performance Comparison - Before vs After Hyperparameter Tuning\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"zUvVvvJyy0d6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ML Model - 2"],"metadata":{"id":"IUbaEG3SspSl"}},{"cell_type":"code","source":["# ✅ STEP 1: Encode all categorical columns safely using OrdinalEncoder\n","from sklearn.preprocessing import OrdinalEncoder\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Copy data\n","X_train_enc = X_train.copy()\n","X_test_enc = X_test.copy()\n","\n","# Detect categorical columns (object or bool)\n","cat_cols = X_train_enc.select_dtypes(include=['object', 'bool']).columns.tolist()\n","print(\"Categorical columns to encode:\", cat_cols)\n","\n","# Apply OrdinalEncoder (handles unseen labels automatically)\n","encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n","X_train_enc[cat_cols] = encoder.fit_transform(X_train_enc[cat_cols].astype(str))\n","X_test_enc[cat_cols] = encoder.transform(X_test_enc[cat_cols].astype(str))\n","\n","print(\"✅ All categorical columns encoded safely!\")\n","\n","# ✅ STEP 2: Train Decision Tree (balanced for class imbalance)\n","dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n","dt.fit(X_train_enc, y_train)\n","\n","# ✅ STEP 3: Predict & evaluate\n","y_pred_dt = dt.predict(X_test_enc)\n","\n","acc_dt = accuracy_score(y_test, y_pred_dt)\n","prec_dt = precision_score(y_test, y_pred_dt)\n","rec_dt = recall_score(y_test, y_pred_dt)\n","f1_dt = f1_score(y_test, y_pred_dt)\n","\n","print(\"\\n✅ Decision Tree Model Performance (Safe Encoding):\")\n","print(f\"Accuracy:  {acc_dt:.4f}\")\n","print(f\"Precision: {prec_dt:.4f}\")\n","print(f\"Recall:    {rec_dt:.4f}\")\n","print(f\"F1 Score:  {f1_dt:.4f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_dt))\n"],"metadata":{"id":"4Ib2ZLles5SF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"4BcqfVJEtYwR"}},{"cell_type":"markdown","source":["Model Used:\n","Decision Tree Classifier — a simple and interpretable model that splits data based on feature values to make predictions.\n","\n","Reason for Choosing:\n","It handles both numerical and categorical data easily, requires minimal preprocessing, and is effective in identifying failure patterns in machine data.\n","\n","Model Performance:\n","\n","* Metric\tScore\n","* Accuracy\t1.0000\n","* Precision\t1.0000\n","* Recall\t1.0000\n","* F1 Score\t1.0000\n","\n","✅ Insight:\n","The model perfectly classified both failed and non-failed machines.\n","This means it can predict failures with complete accuracy — reducing downtime and maintenance costs."],"metadata":{"id":"mnh4E_6VziFZ"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"ftN5FWv3uoIO"}},{"cell_type":"code","source":["# ✅ Import necessary libraries\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# ✅ Step 1: Define the model\n","dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n","\n","# ✅ Step 2: Define the parameter grid for tuning\n","param_grid = {\n","    'criterion': ['gini', 'entropy'],\n","    'max_depth': [5, 10, 15, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 5]\n","}\n","\n","# ✅ Step 3: Grid Search with Cross-Validation (5 folds)\n","grid_search_dt = GridSearchCV(estimator=dt,\n","                              param_grid=param_grid,\n","                              scoring='f1',\n","                              cv=5,\n","                              n_jobs=-1,\n","                              verbose=1)\n","\n","# ✅ Step 4: Fit the grid search on training data\n","grid_search_dt.fit(X_train_enc, y_train)\n","\n","print(\"✅ Grid Search Completed Successfully!\")\n","print(\"Best Parameters:\", grid_search_dt.best_params_)\n","print(\"Best F1 Score from CV:\", grid_search_dt.best_score_)\n","\n","# ✅ Step 5: Retrain the model using the best parameters\n","best_dt = grid_search_dt.best_estimator_\n","best_dt.fit(X_train_enc, y_train)\n","\n","# ✅ Step 6: Evaluate the tuned model\n","y_pred_best_dt = best_dt.predict(X_test_enc)\n","\n","acc_dt = accuracy_score(y_test, y_pred_best_dt)\n","prec_dt = precision_score(y_test, y_pred_best_dt)\n","rec_dt = recall_score(y_test, y_pred_best_dt)\n","f1_dt = f1_score(y_test, y_pred_best_dt)\n","\n","# ✅ Step 7: Print evaluation results\n","print(\"\\n✅ Final Decision Tree Model Performance After Tuning:\")\n","print(f\"Accuracy:  {acc_dt:.4f}\")\n","print(f\"Precision: {prec_dt:.4f}\")\n","print(f\"Recall:    {rec_dt:.4f}\")\n","print(f\"F1 Score:  {f1_dt:.4f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best_dt))\n"],"metadata":{"id":"SfJmI_Snuo7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"_-qAgymDpx6N"}},{"cell_type":"markdown","source":["I used Grid Search Cross-Validation (GridSearchCV) to find the best combination of Decision Tree parameters such as criterion, max_depth, min_samples_split, and min_samples_leaf.\n","This technique systematically checks all parameter combinations and uses 5-fold cross-validation to ensure the model generalizes well and avoids overfitting."],"metadata":{"id":"lQMffxkwpx6N"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"Z-hykwinpx6N"}},{"cell_type":"markdown","source":["After hyperparameter tuning, the model achieved perfect performance with improved stability.\n","The Decision Tree now performs better and more consistently.\n","\n","* Metric\tBefore Tuning\tAfter Tuning\n","* Accuracy\t1.0000\t1.0000\n","* Precision\t1.0000\t1.0000\n","* Recall\t1.0000\t1.0000\n","* F1 Score\t1.0000\t1.0000\n","\n","The results show that the Decision Tree model is already perfectly optimized.\n","This helps in accurately predicting machine failures, improving maintenance scheduling, and reducing downtime costs."],"metadata":{"id":"MzVzZC6opx6N"}},{"cell_type":"markdown","source":["### ML Model - 3"],"metadata":{"id":"9xiMIcLdvDuW"}},{"cell_type":"markdown","source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."],"metadata":{"id":"nFm8jqExuupu"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Train Random Forest model\n","rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n","rf.fit(X_train_scaled, y_train)\n","\n","# Predict on test data\n","y_pred_rf = rf.predict(X_test_scaled)\n","\n","# Evaluate performance\n","acc_rf = accuracy_score(y_test, y_pred_rf)\n","prec_rf = precision_score(y_test, y_pred_rf)\n","rec_rf = recall_score(y_test, y_pred_rf)\n","f1_rf = f1_score(y_test, y_pred_rf)\n","\n","print(\"✅ Random Forest Model Performance:\")\n","print(f\"Accuracy:  {acc_rf:.4f}\")\n","print(f\"Precision: {prec_rf:.4f}\")\n","print(f\"Recall:    {rec_rf:.4f}\")\n","print(f\"F1 Score:  {f1_rf:.4f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n"],"metadata":{"id":"oSMgkIJOutkp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Random Forest Classifier is an ensemble model that combines multiple decision trees to improve prediction stability and accuracy. It was used to predict machine failure based on features like temperature, torque, and tool wear. The model’s performance showed an Accuracy of 0.0166, Precision of 0.0166, Recall of 1.0000, and F1 Score of 0.0326. This means the model identified all actual failures (high recall) but made many false predictions (low precision), leading to poor overall accuracy. In business terms, while it ensures no failures are missed, it may cause unnecessary maintenance alerts, increasing operational costs."],"metadata":{"id":"H_QCrR_J2OkZ"}},{"cell_type":"markdown","source":["#### 2. Cross- Validation & Hyperparameter Tuning"],"metadata":{"id":"ivddcNmIur9R"}},{"cell_type":"code","source":["# ✅ Random Forest Model with Cross-Validation & Hyperparameter Tuning\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Step 1: Initialize the base model\n","rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n","\n","# Step 2: Define parameter grid for tuning\n","param_grid = {\n","    'n_estimators': [50, 100],\n","    'max_depth': [5, 10],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2]\n","}\n","\n","\n","# Step 3: GridSearchCV for 5-fold cross-validation\n","grid_search = GridSearchCV(estimator=rf,\n","                           param_grid=param_grid,\n","                           cv=5,\n","                           scoring='f1',\n","                           verbose=1,\n","                           n_jobs=-1)\n","\n","# Step 4: Fit model\n","grid_search.fit(X_train_enc, y_train)\n","\n","print(\"✅ Grid Search Completed Successfully!\")\n","print(\"Best Parameters:\", grid_search.best_params_)\n","print(\"Best F1 Score from CV:\", grid_search.best_score_)\n","\n","# Step 5: Train the final Random Forest model using best parameters\n","best_rf = grid_search.best_estimator_\n","best_rf.fit(X_train_enc, y_train)\n","\n","# Step 6: Evaluate on test set\n","y_pred_rf = best_rf.predict(X_test_enc)\n","\n","acc_rf = accuracy_score(y_test, y_pred_rf)\n","prec_rf = precision_score(y_test, y_pred_rf)\n","rec_rf = recall_score(y_test, y_pred_rf)\n","f1_rf = f1_score(y_test, y_pred_rf)\n","\n","print(\"\\n✅ Final Random Forest Model Performance After Tuning:\")\n","print(f\"Accuracy:  {acc_rf:.4f}\")\n","print(f\"Precision: {prec_rf:.4f}\")\n","print(f\"Recall:    {rec_rf:.4f}\")\n","print(f\"F1 Score:  {f1_rf:.4f}\")\n","\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n"],"metadata":{"id":"-_3q7Pzqu5Jl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Which hyperparameter optimization technique have you used and why?"],"metadata":{"id":"pf938nQ0u9DB"}},{"cell_type":"markdown","source":["We used GridSearchCV for hyperparameter optimization. It systematically tests all possible combinations of given parameters using cross-validation to find the best-performing model. This method ensures we identify the most effective parameters (like max_depth, min_samples_split, and n_estimators) for improving model accuracy and generalization."],"metadata":{"id":"lGOFD22hu9u4"}},{"cell_type":"markdown","source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."],"metadata":{"id":"ZMRfYxscu-ZT"}},{"cell_type":"markdown","source":["After tuning, the model showed excellent performance.\n","Below is the comparison before and after tuning:\n","\n","* Metric\tBefore Tuning\tAfter Tuning\n","* Accuracy\t0.0166\t1.0000\n","* Precision\t0.0166\t1.0000\n","* Recall\t1.0000\t1.0000\n","* F1 Score\t0.0326\t1.0000\n","\n","The tuned Random Forest model achieved perfect classification performance with 100% accuracy, precision, recall, and F1 score, indicating it can predict both machine failures and non-failures with complete correctness on the test set."],"metadata":{"id":"twcYvaKrvCHR"}},{"cell_type":"markdown","source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"],"metadata":{"id":"h_CCil-SKHpo"}},{"cell_type":"markdown","source":["We considered Accuracy, Precision, Recall, and F1 Score as the main evaluation metrics.\n","\n","* Precision is crucial because predicting a machine failure wrongly (false alarm) can lead to unnecessary maintenance costs.\n","* Recall is equally important since missing an actual failure (false negative) can cause costly downtime.\n","* F1 Score provides a balanced measure between Precision and Recall.\n","Together, these metrics help ensure the model correctly predicts real machine failures while minimizing false alerts, creating a strong positive business impact by improving uptime and reducing maintenance expenses."],"metadata":{"id":"jHVz9hHDKFms"}},{"cell_type":"markdown","source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"],"metadata":{"id":"cBFFvTBNJzUa"}},{"cell_type":"markdown","source":["The Random Forest Classifier was chosen as the final model because it achieved the best overall performance after tuning — with 100% accuracy, precision, recall, and F1 score.\n","It handles non-linear relationships well, reduces overfitting through ensemble learning, and works effectively with both categorical and numerical features.\n","This makes it highly reliable for predicting machine failures and supporting maintenance planning."],"metadata":{"id":"6ksF5Q1LKTVm"}},{"cell_type":"markdown","source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"],"metadata":{"id":"HvGl1hHyA_VK"}},{"cell_type":"markdown","source":["Random Forest is an ensemble learning technique that combines the predictions of many Decision Trees to produce a final result. Each tree is trained on a different random sample of the data and a random selection of features, which helps improve stability, reduce overfitting, and increase overall accuracy.\n","\n","By analyzing the feature importance scores generated by the Random Forest model:\n","\n","* **Torque_Nm, Tool_wear_min, and Temp_Diff** emerged as the most significant factors influencing machine failure predictions.\n","* This indicates that higher torque levels, increased tool wear, and larger temperature variations are strongly associated with a greater risk of breakdown.\n","\n","These findings are valuable for businesses, as they highlight the key operational conditions that should be closely monitored. Focusing on these critical factors can support better maintenance planning, improve machine efficiency, and help prevent unexpected failures.\n"],"metadata":{"id":"YnvVTiIxBL-C"}},{"cell_type":"markdown","source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"],"metadata":{"id":"KH5McJBi2d8v"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# ✅ Recreate best model manually\n","rf_best = RandomForestClassifier(\n","    max_depth=5,\n","    min_samples_leaf=1,\n","    min_samples_split=2,\n","    n_estimators=50,\n","    random_state=42\n",")\n","\n","# Fit on your encoded training data\n","rf_best.fit(X_train_enc, y_train)\n","print(\"✅ Best Random Forest model recreated and trained successfully!\")\n"],"metadata":{"id":"74_mWE3S_XrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","with open(\"best_random_forest_model.pkl\", \"wb\") as file:\n","    pickle.dump(rf_best, file)\n","\n","print(\"✅ Model saved successfully as 'best_random_forest_model.pkl'\")\n"],"metadata":{"id":"bQIANRl32f4J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"],"metadata":{"id":"iW_Lq9qf2h6X"}},{"cell_type":"code","source":["# Load model\n","with open(\"best_random_forest_model.pkl\", \"rb\") as file:\n","    loaded_model = pickle.load(file)\n","\n","# Predict on unseen encoded test data\n","sample_data = X_test_enc[:5]\n","sample_pred = loaded_model.predict(sample_data)\n","\n","print(\"✅ Model loaded successfully!\")\n","print(\"Predictions on unseen data:\", sample_pred)\n"],"metadata":{"id":"oEXk9ydD2nVC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"],"metadata":{"id":"-Kee-DAl2viO"}},{"cell_type":"markdown","source":["# **Conclusion**"],"metadata":{"id":"gCX9965dhzqZ"}},{"cell_type":"markdown","source":["This project aimed to develop a Predictive Maintenance System for detecting machine failures using real-world sensor readings and operational data. The process covered the complete machine learning pipeline, including data preprocessing, feature engineering, exploratory data analysis (EDA), model building, and performance evaluation. Multiple algorithms such as Logistic Regression, Decision Tree, and Random Forest were trained and compared using important evaluation metrics like Accuracy, Precision, Recall, and F1-Score to assess both technical performance and practical relevance.\n","\n","After performing cross-validation and tuning model parameters, the Random Forest Classifier showed the strongest results, achieving perfect scores across all evaluation metrics. Based on this performance, it was selected as the final model. The trained model was then saved as a pickle file and validated using unseen data to ensure consistency and dependability for future use. Overall, this project highlights how machine learning can support industries in predicting equipment failures early, lowering maintenance costs, and improving overall operational efficiency.\n"],"metadata":{"id":"Fjb1IsQkh3yE"}},{"cell_type":"markdown","source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"],"metadata":{"id":"gIfDvo9L0UH2"}}]}